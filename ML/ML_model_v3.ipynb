{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from ipython_config import RDS_pwd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import ComplementNB,GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Lasso, LogisticRegression, LinearRegression \n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pickle\n",
    "import os\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import h5py\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database (Note: The package psychopg2 is required for Postgres to work with SQLAlchemy)\n",
    "db_string = f\"postgres://postgres:{RDS_pwd}@platinum-rds.cbu3an3ywyth.us-east-2.rds.amazonaws.com/Platinum_Lyrics\"\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing names of the tables present in the database\n",
    "print(engine.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyrics_features = engine.table_names()[0]\n",
    "lyrics_only = engine.table_names()[7]\n",
    "lyrics_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SQL database table into a DataFrame.\n",
    "lyrics_only_df = pd.read_sql_table(lyrics_only,engine)\n",
    "lyrics_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the prefix \"word_\":\n",
    "column_names = lyrics_only_df.columns.values\n",
    "new_column_names = {}\n",
    "for column in column_names:\n",
    "                new_column_names[column] = column.replace('word_', '')\n",
    "lyrics_only_df = lyrics_only_df.rename(columns=new_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of lyrics_only_df \n",
    "lyrics_only_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of columns\n",
    "list(lyrics_only_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns with null values\n",
    "null_columns=lyrics_only_df.columns[lyrics_only_df.isnull().any()]\n",
    "lyrics_only_df[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we don't need \n",
    "lyrics_only_df.drop(columns=[\"track_id\",\"artist_name\",\n",
    "                                              \"song_title\",\"song_year\",\"target_weeks\",\n",
    "                                              \"target_peak\"],axis=1,inplace=True)\n",
    "lyrics_only_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove some noisy features found through previous runs of the model\n",
    "lyrics_only_df.drop(columns=[\"que\",\"the\",\"ooh\",\"con\",\"tri\",\"una\",\n",
    "                            \"por\",\"noth\",\"mai\",\"whi\",\"como\",\"qui\",\n",
    "                            \"qui\", \"das\", \"doe\", \"der\", \"des\", \"dan\",\n",
    "                             \"che\", \"mani\", \"vida\", \"mit\", \"pas\", \"per\",\n",
    "                             \"cos\", \"dri\", \"mir\", \"nos\", \"dir\", \"poi\", \"voi\",\n",
    "                             \"och\", \"ver\", \"mari\", \"har\", \"doo\", \"ima\"],\n",
    "                    axis=1,inplace=True)\n",
    "lyrics_only_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get data types \n",
    "for dtype in lyrics_only_df.dtypes: \n",
    "    print(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to test and train\n",
    "X = lyrics_only_df.drop(columns=['target_success'],axis=1)\n",
    "y = lyrics_only_df['target_success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 1 - Sequential Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential backward selection(sbs)\n",
    "# sbs = SFS(LinearRegression(), \n",
    "#           k_features=100, \n",
    "#           forward=False, \n",
    "#           floating=False,\n",
    "#           cv=0)\n",
    "# sbs.fit(X, y)\n",
    "# sbs.k_feature_names_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample \n",
    "X_resampled, y_resampled = SMOTE(random_state=1).fit_resample(\n",
    "    X_train, y_train\n",
    ")\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler= scaler.fit(X_resampled)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_resampled)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 2 - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PCA model\n",
    "# pca = PCA(n_components=3)\n",
    "# X_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_pca_df = pd.DataFrame(data=X_pca,columns=[\"principal component 1\", \"principal component 2\", \"principal component 3\"])\n",
    "# X_pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the explained variance\n",
    "# pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METHOD 3 - Lasso Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features using Lasso regularization using SelectFromModel\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1,fit_intercept=False,penalty='l2'))\n",
    "sel_.fit(scaler.transform(X_resampled), y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising features that were kept by the lasso regularisation\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of with the selected features\n",
    "selected_feat = X_resampled.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((X_resampled.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features which coefficient was shrank to zero \n",
    "np.sum(sel_.estimator_.coef_ == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the removed features\n",
    "removed_feats = X_resampled.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new X test and train with 579 features\n",
    "X_train_selected = sel_.transform(X_resampled.fillna(0))\n",
    "X_test_selected = sel_.transform(X_test.fillna(0))\n",
    "X_train_selected.shape, X_test_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 -  ComplementNB Naive Bayes (with Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create classifier  \n",
    "compNB_model = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the classifier \n",
    "compNB_model.fit(X_train_selected, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = compNB_model.predict(X_test_selected)\n",
    "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model's accuracy\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 -  GaussianNB Naive Bayes (with Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier\n",
    "GNB_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier \n",
    "GNB_model.fit(X_train_selected, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = GNB_model.predict(X_test_selected)\n",
    "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model's accuracy\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - Deep Neural Net model (with Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train_selected[0])\n",
    "hidden_nodes_layer1 = 10\n",
    "# hidden_nodes_layer2 = 5\n",
    "\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1,input_dim= number_input_features, activation=\"relu\"))\n",
    "\n",
    "# # Second hidden layer \n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1,activation=\"sigmoid\"))\n",
    "\n",
    "# Restore the model weights\n",
    "#nn.load_weights(\"checkpoints/weights.100.hdf5\")\n",
    "\n",
    "# Check the structure of the model \n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change y_train and y_test type for tf\n",
    "y_resampled_np = np.array(y_resampled)\n",
    "y_test_np = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_selected,y_resampled_np,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_selected,y_test_np,verbose=2)\n",
    "print(f\"Test Loss: {model_loss}, Test Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = nn.evaluate(X_train_selected,y_resampled_np,verbose=2)\n",
    "print(f\"Train Loss: {model_loss}, Train Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 - Random Forest Classifier (With Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=50,random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "rf_model.fit(X_train_selected, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = rf_model.predict(X_test_selected)\n",
    "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model's accuracy\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance in the Random Forest model\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the NB model as a pickle in a file \n",
    "joblib.dump(compNB_model, 'NB_model_v3.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the nn model as an h5 file\n",
    "nn.save(\"nn_model_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the NB model as a pickle in a file \n",
    "joblib.dump(rf_model, 'rf_model_joblib_v2.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the NB model as a pickle in a file \n",
    "pickle.dump(rf_model, open('rf_model_pickle_v2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
